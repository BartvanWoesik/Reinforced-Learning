{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "env = gym.make('SpaceInvaders-v4')\r\n",
    "height, width, channels = env.observation_space.shape\r\n",
    "actions = env.action_space.n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "env.unwrapped.get_action_meanings()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "episodes = 5\r\n",
    "for episode in range(1, episodes+1):\r\n",
    "    state = env.reset()\r\n",
    "    done = False\r\n",
    "    score = 0\r\n",
    "    \r\n",
    "    while not done:\r\n",
    "        env.render()\r\n",
    "        action = random.choice([0,1,2,3,4,5])\r\n",
    "        n_state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print('Episode:{} score:{}'.format(episode, score))\r\n",
    "env.close()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 score:105.0\n",
      "Episode:2 score:110.0\n",
      "Episode:3 score:215.0\n",
      "Episode:4 score:150.0\n",
      "Episode:5 score:245.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def build_model(height, width, channels, actions):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Convolution2D(32, (8,8), strides=(6,6), activation='relu', input_shape=(3,height,width,channels)))\r\n",
    "\r\n",
    "    model.add(Flatten())\r\n",
    " \r\n",
    "    model.add(Dense(32, activation = 'relu'))\r\n",
    "    model.add(Dense(actions, activation = 'linear'))\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "model = build_model(height, width, channels, actions)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def build_agent(model, actions):\r\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\r\n",
    "                                  attr=\"eps\", value_max = 1.,\r\n",
    "                                  value_min = .1,\r\n",
    "                                  value_test = 2,\r\n",
    "                                  nb_steps = 10000\r\n",
    "                                 )\r\n",
    "    \r\n",
    "    memory = SequentialMemory(limit =100, window_length =3)\r\n",
    "    \r\n",
    "    dqn = DQNAgent(model = model,\r\n",
    "                   memory=memory,\r\n",
    "                   policy=policy, \r\n",
    "                   enable_dueling_network = True,\r\n",
    "                   dueling_type = 'avg',\r\n",
    "                   nb_actions = actions,\r\n",
    "                   nb_steps_warmup =1000\r\n",
    "                  )\r\n",
    "    return dqn\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "dqn = build_agent(model, actions)\r\n",
    "dqn.compile(Adam(lr=1e-4))\r\n",
    "dqn.fit(env, nb_steps = 10000, visualize=False, verbose =1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1074/10000 [==>...........................] - ETA: 8:29 - reward: 0.2095done, took 61.914 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28809912988>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}