{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import random\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class ShowerEnv(Env):\r\n",
    "    def __init__(self):\r\n",
    "        self.action_space = Discrete(3)\r\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\r\n",
    "        self.state = 38 + random.randint(-3,3)\r\n",
    "        self.shower_lenght =60\r\n",
    "        print(state)\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        self.state += (action -1)*2\r\n",
    "        self.shower_lenght -=1\r\n",
    "\r\n",
    "\r\n",
    "        if self.state >= 36 and self.state <= 40:\r\n",
    "            reward =1\r\n",
    "        else:\r\n",
    "            reward = -1\r\n",
    "\r\n",
    "        if self.shower_lenght <= 0:\r\n",
    "            done = True\r\n",
    "        else:\r\n",
    "            done = False\r\n",
    "\r\n",
    "        self.state += random.randint(-1,1)\r\n",
    "        info = {}\r\n",
    "\r\n",
    "        return self.state, reward, done, info\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "    def reset(self):\r\n",
    "        self.state = 38 + random.randint(-3,3)\r\n",
    "        self.shower_lenght =60\r\n",
    "        return self.state\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "env = ShowerEnv()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "38\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "episodes = 10\r\n",
    "for episode in range(1, episodes+1):\r\n",
    "    state = env.reset()\r\n",
    "    done = False\r\n",
    "    score = 0\r\n",
    "    \r\n",
    "    while not done:\r\n",
    "        env.render()\r\n",
    "        action = env.action_space.sample()\r\n",
    "        n_state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print('Episode:{} score:{}'.format(episode, score))\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 score:-52\n",
      "Episode:2 score:-44\n",
      "Episode:3 score:-54\n",
      "Episode:4 score:-38\n",
      "Episode:5 score:18\n",
      "Episode:6 score:-52\n",
      "Episode:7 score:-22\n",
      "Episode:8 score:-60\n",
      "Episode:9 score:-42\n",
      "Episode:10 score:-44\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "states = env.observation_space.shape\r\n",
    "print(states)\r\n",
    "actions = env.action_space.n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def build_model(states, actions):\r\n",
    "    model = Sequential()\r\n",
    "    \r\n",
    "    model.add(Dense(48, activation='relu', input_shape = states))\r\n",
    "    \r\n",
    "    model.add(Dense(21, activation = 'relu'))\r\n",
    "    model.add(Dense(actions, activation = 'linear'))\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "model = build_model(states, actions)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def build_agent(model, actions):\r\n",
    "    policy = BoltzmannQPolicy()\r\n",
    "    \r\n",
    "    memory = SequentialMemory(limit =100, window_length =1)\r\n",
    "    \r\n",
    "    dqn = DQNAgent(model = model,\r\n",
    "                   memory=memory,\r\n",
    "                   policy=policy, \r\n",
    "                   \r\n",
    "                   nb_actions = actions,\r\n",
    "                   nb_steps_warmup =100,\r\n",
    "                   target_model_update=1e-2\r\n",
    "                  )\r\n",
    "    return dqn\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "dqn = build_agent(model, actions)\r\n",
    "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\r\n",
    "dqn.fit(env, nb_steps = 100000, visualize=False, verbose =1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1187/10000 [==>...........................] - ETA: 50s - reward: -0.8147done, took 6.858 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x251ebcb0508>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\r\n",
    "print(np.mean(scores.history['episode_reward']))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}