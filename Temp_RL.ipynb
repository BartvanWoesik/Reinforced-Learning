{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Template for RL:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Import different libarys for data transformation and model creation\r\n",
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\r\n",
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import random\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create custum environment for RL\r\n",
    "\r\n",
    "class CustomEnv(Env):\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create instance of custom environment\r\n",
    "env = CustomEnv()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#build model for agent\r\n",
    "def build_model(states, actions):\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creatate agent\r\n",
    "def build_agent(model, actions):\r\n",
    "    policy = {}\r\n",
    "    memory = {}\r\n",
    "    dqn = DQNAgent( model = {},\r\n",
    "                    memory = {},\r\n",
    "                    policy = {},\r\n",
    "                    nb_actions = actions,\r\n",
    "                    nb_steps_warmup = {})\r\n",
    "    return dqn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create instance of agent and train agent\r\n",
    "dqn = build_agent(model, actions)\r\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mae'])\r\n",
    "dqn.fit(env, nb_steps = 100000, visualize=False, verbose =1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}