{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class ShowerEnv(Env):\r\n",
    "    def __init__(self):\r\n",
    "        self.action_space = Discrete(3)\r\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\r\n",
    "        self.state = 38 + random.randint(-3,3)\r\n",
    "        self.shower_lenght =60\r\n",
    "\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        self.state += (action -1)*2\r\n",
    "        self.shower_lenght -=1\r\n",
    "\r\n",
    "\r\n",
    "        if self.state >= 37 and self.state <= 39:\r\n",
    "            reward =1\r\n",
    "        else:\r\n",
    "            reward = -1\r\n",
    "\r\n",
    "        if self.shower_lenght <= 0:\r\n",
    "            done = True\r\n",
    "        else:\r\n",
    "            done = False\r\n",
    "\r\n",
    "        self.state += random.randint(-1,1)\r\n",
    "        info = {}\r\n",
    "\r\n",
    "        return self.state, reward, done, info\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "    def reset(self):\r\n",
    "        self.state = 38 + random.randint(-3,3)\r\n",
    "        self.shower_lenght =60\r\n",
    "        return self.state\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "env = ShowerEnv()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\bartw\\.conda\\envs\\games_rl\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "episodes = 10\r\n",
    "for episode in range(1, episodes+1):\r\n",
    "    state = env.reset()\r\n",
    "    done = False\r\n",
    "    score = 0\r\n",
    "    \r\n",
    "    while not done:\r\n",
    "        env.render()\r\n",
    "        action = env.action_space.sample()\r\n",
    "        n_state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print('Episode:{} score:{}'.format(episode, score))\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 score:-54\n",
      "Episode:2 score:-58\n",
      "Episode:3 score:-16\n",
      "Episode:4 score:-58\n",
      "Episode:5 score:-22\n",
      "Episode:6 score:-46\n",
      "Episode:7 score:-50\n",
      "Episode:8 score:-58\n",
      "Episode:9 score:-8\n",
      "Episode:10 score:-50\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "states = env.observation_space.shape\r\n",
    "actions = env.action_space.n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def build_model(states, actions):\r\n",
    "    model = Sequential()\r\n",
    "    \r\n",
    "    model.add(Dense(24, activation='relu', input_shape = states))\r\n",
    "    model.add(Dense(24, activation = 'relu'))\r\n",
    "    model.add(Dense(actions, activation = 'linear'))\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "model = build_model(states, actions)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def build_agent(model, actions):\r\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\r\n",
    "                                  attr=\"eps\", value_max = 1.,\r\n",
    "                                  value_min = .1,\r\n",
    "                                  value_test = 2,\r\n",
    "                                  nb_steps = 10000\r\n",
    "                                 )\r\n",
    "    \r\n",
    "    memory = SequentialMemory(limit =100, window_length =1)\r\n",
    "    \r\n",
    "    dqn = DQNAgent(model = model,\r\n",
    "                   memory=memory,\r\n",
    "                   policy=policy, \r\n",
    "                   enable_dueling_network = True,\r\n",
    "                   dueling_type = 'avg',\r\n",
    "                   nb_actions = actions,\r\n",
    "                   nb_steps_warmup =1000\r\n",
    "                  )\r\n",
    "    return dqn\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "dqn = build_agent(model, actions)\r\n",
    "dqn.compile(Adam(lr=1e-4))\r\n",
    "dqn.fit(env, nb_steps = 10000, visualize=False, verbose =1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From C:\\Users\\bartw\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -0.7130\n",
      "done, took 41.565 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2679b5a5948>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}