{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import random\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "class ShowerEnv(Env):\r\n",
    "\r\n",
    "    def Get_state(self, time, combi):\r\n",
    "        combi[\"dif_x\"] = combi[\"Open_x\"] - combi[\"Close_x\"]\r\n",
    "        combi[\"dif_y\"] = combi[\"Open_y\"] - combi[\"Close_y\"]\r\n",
    "\r\n",
    "        combi = combi.dropna()\r\n",
    "\r\n",
    "        tot_labels    = []\r\n",
    "        nsdq_train   = []\r\n",
    "        spy_train = []\r\n",
    "\r\n",
    "        sum_ = 0 \r\n",
    "\r\n",
    "        for x in range (1, len(combi)):\r\n",
    "            # if(combi.iloc[x][\"Open_x\"] - combi.iloc[x][\"Close_x\"] >=\r\n",
    "            #     combi.iloc[x][\"Open_y\"] - combi.iloc[x][\"Close_y\"]):\r\n",
    "            #     tot_labels.append([1])\r\n",
    "            if(combi.iloc[x][\"Open_x\"] > combi.iloc[x][\"Close_x\"] ):\r\n",
    "                tot_labels.append([1])\r\n",
    "                sum_ = sum_ +1\r\n",
    "            else:\r\n",
    "                tot_labels.append([0]) \r\n",
    "                sum_ = sum_ -1\r\n",
    "            temp_list_x =norm_fun( [combi.iloc[x-1][\"Open_x\"], combi.iloc[x-1][\"Close_x\"], \r\n",
    "                                    combi.iloc[x-1][\"High_x\"], combi.iloc[x-1][\"Low_x\"], \r\n",
    "                                    combi.iloc[x][\"Open_x\"]])\r\n",
    "            temp_list_y = norm_fun([combi.iloc[x-1][\"Open_y\"], combi.iloc[x-1][\"Close_y\"], \r\n",
    "                                    combi.iloc[x-1][\"High_y\"], combi.iloc[x-1][\"Low_y\"], \r\n",
    "                                    combi.iloc[x][\"Open_y\"]] )\r\n",
    "\r\n",
    "            \r\n",
    "            nsdq_train.append( temp_list_x )\r\n",
    "            spy_train.append( temp_list_y )\r\n",
    "\r\n",
    "        \r\n",
    "        nsdq_train = tf.keras.utils.normalize(nsdq_train, axis =1)\r\n",
    "        spy_train = tf.keras.utils.normalize(spy_train, axis=1)\r\n",
    "\r\n",
    "        tot_train = np.array(np.concatenate((nsdq_train,spy_train), axis = 1))\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        tot_labels = np.array(tot_labels)\r\n",
    "        encoder = LabelEncoder()\r\n",
    "        encoder.fit(tot_labels)\r\n",
    "        encoded_Y = encoder.transform(tot_labels)\r\n",
    "        x_out = tot_train[time]\r\n",
    "        y_out = encoded_Y[time]\r\n",
    "\r\n",
    "        return x_out, y_out\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "\r\n",
    "        self.nsdq    = pd.read_csv(\"NQ=F (2).csv\")\r\n",
    "        self.spy     = pd.read_csv(\"SPY (1).csv\")\r\n",
    "        self.time = 0\r\n",
    "\r\n",
    "        combi   = pd.merge(self.nsdq, self.spy, on = 'Date')\r\n",
    "        print(combi)\r\n",
    "        print(self.spy)\r\n",
    "        print(self.nsdq)\r\n",
    "        \r\n",
    "        self.action_space = Discrete(2)\r\n",
    "\r\n",
    "        self.state = [[],[]]\r\n",
    "\r\n",
    "\r\n",
    "        \r\n",
    "\r\n",
    "    def norm_fun(x): \r\n",
    "    \r\n",
    "        high    = x[0]\r\n",
    "        low     = x[0]\r\n",
    "        new_x = []\r\n",
    "\r\n",
    "        for n in range(0, len(x)):\r\n",
    "            if ( x[n] > high):\r\n",
    "                high = x[n]\r\n",
    "            if ( x[n] < low):\r\n",
    "                low = x[n]\r\n",
    "                \r\n",
    "        for n in range(0, len(x)):\r\n",
    "            new_x.append((x[n] -low) - (high - low))\r\n",
    "\r\n",
    "        return new_x\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        self.time +=1\r\n",
    "        self.state = self.Get_state( self.time, self.combi)\r\n",
    "\r\n",
    "        if(state.y_out == action):\r\n",
    "            reward = 1\r\n",
    "        else:\r\n",
    "            reward = -1\r\n",
    "\r\n",
    "\r\n",
    "        if time >= 1000:\r\n",
    "            done = True\r\n",
    "        else:\r\n",
    "            done = False\r\n",
    "\r\n",
    "        return self.state, reward, done, info\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "    def reset(self):\r\n",
    "        self.time = 0\r\n",
    "        return self.state\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "env = ShowerEnv()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, Open_x, High_x, Low_x, Close_x, Adj Close_x, Volume_x, Open_y, High_y, Low_y, Close_y, Adj Close_y, Volume_y]\n",
      "Index: []\n",
      "            Date        Open        High         Low       Close   Adj Close  \\\n",
      "0     1995-01-19   46.828125   46.875000   46.687500   46.718750   28.872732   \n",
      "1     1995-01-20   46.656250   46.671875   46.453125   46.546875   28.766497   \n",
      "2     1995-01-23   46.234375   46.687500   46.203125   46.687500   28.853416   \n",
      "3     1995-01-24   46.671875   46.750000   46.640625   46.750000   28.892042   \n",
      "4     1995-01-25   46.515625   47.046875   46.515625   46.875000   28.969303   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "6665  2021-07-12  435.429993  437.350006  434.970001  437.079987  437.079987   \n",
      "6666  2021-07-13  436.239990  437.839996  435.309998  435.589996  435.589996   \n",
      "6667  2021-07-14  437.399994  437.920013  434.910004  436.239990  436.239990   \n",
      "6668  2021-07-15  434.809998  435.529999  432.720001  434.750000  434.750000   \n",
      "6669  2021-07-16  436.010010  436.059998  430.920013  431.339996  431.339996   \n",
      "\n",
      "        Volume  \n",
      "0       139100  \n",
      "1        78700  \n",
      "2        53700  \n",
      "3        32400  \n",
      "4        15700  \n",
      "...        ...  \n",
      "6665  52889600  \n",
      "6666  52911300  \n",
      "6667  64130400  \n",
      "6668  55126400  \n",
      "6669  75784700  \n",
      "\n",
      "[6670 rows x 7 columns]\n",
      "           Date      Open      High       Low     Close  Adj Close    Volume\n",
      "0     11/2/2015   4644.50   4699.50   4625.00   4694.00    4694.00  162655.0\n",
      "1     11/3/2015   4688.25   4727.50   4677.00   4712.00    4712.00  186758.0\n",
      "2     11/4/2015   4711.75   4729.75   4691.25   4709.50    4709.50  231004.0\n",
      "3     11/5/2015   4708.25   4727.50   4673.75   4695.25    4695.25  246167.0\n",
      "4     11/6/2015   4700.75   4708.75   4658.75   4703.00    4703.00  276682.0\n",
      "...         ...       ...       ...       ...       ...        ...       ...\n",
      "1431  7/12/2021  14824.00  14888.50  14785.00  14869.25   14869.25  362608.0\n",
      "1432  7/13/2021  14880.00  14994.75  14794.50  14865.00   14865.00  506392.0\n",
      "1433  7/14/2021  14869.75  14996.00  14841.25  14891.75   14891.75  499191.0\n",
      "1434  7/15/2021  14895.00  14962.75  14708.25  14787.50   14787.50  634785.0\n",
      "1435  7/16/2021  14778.50  14869.50  14656.00  14670.50   14670.50  634785.0\n",
      "\n",
      "[1436 rows x 7 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "episodes = 10\r\n",
    "for episode in range(1, episodes+1):\r\n",
    "    state = env.reset()\r\n",
    "    done = False\r\n",
    "    score = 0\r\n",
    "    \r\n",
    "    while not done:\r\n",
    "        env.render()\r\n",
    "        action = env.action_space.sample()\r\n",
    "        n_state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print('Episode:{} score:{}'.format(episode, score))\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'ShowerEnv' object has no attribute 'combi'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-4f2bd0e52934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mn_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode:{} score:{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-c654134189b0>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGet_state\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_out\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ShowerEnv' object has no attribute 'combi'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "states = env.observation_space.shape\r\n",
    "actions = env.action_space.n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model(states, actions):\r\n",
    "    model = Sequential()\r\n",
    "    \r\n",
    "    model.add(Dense(48, activation='relu', input_shape = states))\r\n",
    "    \r\n",
    "    model.add(Dense(21, activation = 'relu'))\r\n",
    "    model.add(Dense(actions, activation = 'linear'))\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = build_model(states, actions)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_agent(model, actions):\r\n",
    "    policy = BoltzmannQPolicy()\r\n",
    "    \r\n",
    "    memory = SequentialMemory(limit =100, window_length =1)\r\n",
    "    \r\n",
    "    dqn = DQNAgent(model = model,\r\n",
    "                   memory=memory,\r\n",
    "                   policy=policy, \r\n",
    "                   \r\n",
    "                   nb_actions = actions,\r\n",
    "                   nb_steps_warmup =100,\r\n",
    "                   target_model_update=1e-2\r\n",
    "                  )\r\n",
    "    return dqn\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dqn = build_agent(model, actions)\r\n",
    "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\r\n",
    "dqn.fit(env, nb_steps = 100000, visualize=False, verbose =1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\r\n",
    "print(np.mean(scores.history['episode_reward']))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}